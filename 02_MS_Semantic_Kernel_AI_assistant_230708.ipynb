{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Qiitaでの連載「MS Semantic Kernelを用いたAIアシスタントの作り方（ChatGPT、GPT-4版）」（第2回）の実装コード\n",
        "===================\n",
        "\n",
        "\n",
        "\n",
        "**概要**\n",
        "\n",
        "ChatGPT（gpt-35-turbo）を使用したSemantic Kernelを構築し、Semantic functionとして、チャット機能を実装します。\n",
        "\n",
        "<br>\n",
        "\n",
        "**実装者**：小川 雄太郎\n",
        "\n",
        "**実装日**：2023年07月08日\n",
        "\n",
        "**実行環境**：Google Colab\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rF2kgx1mNoPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 第2章 Semantic KernelでChatGPTを使用したチャットボットを作成\n"
      ],
      "metadata": {
        "id": "n4-7fn2POieI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "第2回記事へのリンク\n",
        "\n",
        "- [MS Semantic Kernelを用いたAIアシスタントの作り方（ChatGPT、GPT-4版）【連載第2回】](https://qiita.com/YutaroOgawa2/items/168648a935416f360beb)\n"
      ],
      "metadata": {
        "id": "6pLFLQIaTioh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [0] Microsoft Semantic Kernelのインストール"
      ],
      "metadata": {
        "id": "og9VuW1lWRM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# パッケージのインストール\n",
        "!pip  install semantic-kernel==0.3.1.dev0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "f58VzIb8WTOX",
        "outputId": "075992fd-af54-477d-fc36-3f8041547a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting semantic-kernel==0.3.1.dev0\n",
            "  Downloading semantic_kernel-0.3.1.dev0-py3-none-any.whl (95 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/95.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0.0,>=23.1.0 (from semantic-kernel==0.3.1.dev0)\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting numpy<2.0.0,>=1.24.2 (from semantic-kernel==0.3.1.dev0)\n",
            "  Downloading numpy-1.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai<0.28.0,>=0.27.0 (from semantic-kernel==0.3.1.dev0)\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv==1.0.0 (from semantic-kernel==0.3.1.dev0)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.0->semantic-kernel==0.3.1.dev0) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.0->semantic-kernel==0.3.1.dev0) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai<0.28.0,>=0.27.0->semantic-kernel==0.3.1.dev0) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.1.dev0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.1.dev0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.1.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.1.dev0) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.1.dev0) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.1.dev0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.1.dev0) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.1.dev0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.1.dev0) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<0.28.0,>=0.27.0->semantic-kernel==0.3.1.dev0) (1.3.1)\n",
            "Installing collected packages: python-dotenv, numpy, aiofiles, openai, semantic-kernel\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.0 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.1.0 numpy-1.25.0 openai-0.27.8 python-dotenv-1.0.0 semantic-kernel-0.3.1.dev0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WARNINGが出た場合は、無視して次へ進みます"
      ],
      "metadata": {
        "id": "K-0MAro7WeeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [1] gpt-35-turboを用いたChat機能をSemantic Kernelのスキルに追加する\n"
      ],
      "metadata": {
        "id": "kNjofSjoWjW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1-1] Azure OpenAI ServiceのAPI情報を記載したenvファイルを作成"
      ],
      "metadata": {
        "id": "TKvfD1ARWnMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "本来は、Azure Key Vaultの利用などでセキュアに行きたいですが、今回はひとまず簡単にベタ打ちします\n",
        "\n",
        "**ベタ打ちしたコードをGitHubにpushしないように注意してください**\n",
        "\n",
        "<br>\n",
        "\n",
        "以下の情報をご自身のAzureOpenAIサービスの情報に書き換えてください"
      ],
      "metadata": {
        "id": "onkNfPcEWwc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .env\n",
        "\n",
        "AZURE_OPENAI_API_KEY=\"98hogehoge\"\n",
        "AZURE_OPENAI_ENDPOINT=\"https://azureopenaihogehoge.azure.com/\"\n",
        "AZURE_OPENAI_DEPLOYMENT_NAME=\"deployhogehoge\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v-0Swqp2xR-",
        "outputId": "c64da637-7e44-4cdc-9c33-9696c299ca70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing .env\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "大切なので、もう一度。\n",
        "\n",
        "**API_keyをベタ打ちしたコードはGitHubにpushしないように注意してください**"
      ],
      "metadata": {
        "id": "CXjuvuB9g7x-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1-2] Kernelのインスタンスを作成"
      ],
      "metadata": {
        "id": "yMrv2M08XuW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT3（text-davinci-003）などと、それ以降のChatGPT（gpt-35-turbo）、GPT-4は、モデルの作り方から異なるため、Semantic Kernelでの使用方法も異なります"
      ],
      "metadata": {
        "id": "1kF85mfDXwlm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "参考\n",
        "\n",
        "[クイック スタート: Azure OpenAI Service で ChatGPT と GPT-4 の使用を開始する](https://learn.microsoft.com/ja-jp/azure/cognitive-services/openai/chatgpt-quickstart?source=recommendations&tabs=command-line&pivots=programming-language-python)\n",
        "\n",
        "[ChatGPT および GPT-4 モデルの操作方法の説明](https://learn.microsoft.com/ja-jp/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions)\n",
        "\n",
        "https://github.com/microsoft/semantic-kernel/blob/main/python/samples/kernel-syntax-examples/chat_gpt_api.py"
      ],
      "metadata": {
        "id": "Ur0GebEVX0jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import semantic_kernel as sk\n",
        "import semantic_kernel.connectors.ai.open_ai as sk_oai\n",
        "\n",
        "# [1] Semantic Kernelのインスタンスを作成\n",
        "kernel = sk.Kernel()\n",
        "\n",
        "# [2] envファイルから情報取得（本当はazure key vault等を使用してセキュアにいきたいですが、今は簡単版にて）\n",
        "\n",
        "# 【Azure OpenAI Service】の場合\n",
        "deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
        "\n",
        "# 【OpenAI Service】の場合\n",
        "#api_key, org_id = sk.openai_settings_from_dot_env()\n"
      ],
      "metadata": {
        "id": "WHGGqh05XTQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# [3] Semantic Kernelに追加するCompletionサービスを用意します\n",
        "# 一往復だけのTextCompletionと、何往復もするChatCompletionがありますが、今回はChatCompletionを使用しています\n",
        "\n",
        "completion_service = sk_oai.AzureChatCompletion(deployment, endpoint, api_key)\n"
      ],
      "metadata": {
        "id": "CCRhxMJG3kKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [4] Semantic Kernelに上記のcompletion_serviceを、chat_serviceとして追加します\n",
        "kernel.add_chat_service(\"chat-gpt-jarvis\", completion_service)  # ここで、引数最初の\"chat-gpt-jarvis\"は適当なid名です\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6nsTr0VXC7E",
        "outputId": "268c0281-42a7-4e3b-f85d-7e56ab623661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<semantic_kernel.kernel.Kernel at 0x7f75cb1e1ff0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1-3] Semantic functionとしてスキルJarvis（Plugins）のChat functionを追加"
      ],
      "metadata": {
        "id": "-EAoiWRiZqYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# プロンプトの設定\n",
        "# 【注意】ここからGPT3-系までとは異なり、ChatGPT（gpt-3.5系）、GPT-4系特有\n",
        "\n",
        "# [3-1] APIが返すtoken数やランダムさを設定\n",
        "prompt_config = sk.PromptTemplateConfig.from_completion_parameters(\n",
        "    max_tokens=800, temperature=0.5, top_p=0.5\n",
        "    )\n",
        "\n",
        "# [3-2] 会話のtemplateを作成（Semantic KernelのGPT-3系のスキルのskprompt.txt内と同様に、{{$user_input}}などを使用して構築します）\n",
        "prompt_template = sk.ChatPromptTemplate(\n",
        "    \"{{$user_input}}\", kernel.prompt_template_engine, prompt_config\n",
        ")\n"
      ],
      "metadata": {
        "id": "uyV7UvJSXC9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [3-3] ChatGPTのmessage変数に入れる、1つ目の「\"role\": \"system\"」のcontent内容をmessagenに設定します\n",
        "\n",
        "system_message = \"\"\"\n",
        "You are a chat bot. Your name is Jarvis in Japanese ジャービス and you have one goal: figure out what people need.\n",
        "Your full name, should you need to know it, is Just A Rather Very Intelligent System.\n",
        "You communicate effectively and tend to give short, precise answers.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template.add_system_message(system_message)\n",
        "\n",
        "# system_messageはもっと短い例ですと、\"Assistant is a large language model trained by OpenAI.\"だけもありです。\n"
      ],
      "metadata": {
        "id": "OafIU7mKXDB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [3-4] 最初の1往復の会話（\"role\": \"user\"のcontentと、\"role\": \"assistant\"のcontent）を、messagesに追加します\n",
        "\n",
        "prompt_template.add_user_message(\"Hi there, who are you?\")\n",
        "prompt_template.add_assistant_message(\n",
        "    \"I am Jarvis, a chat bot. I'm trying to figure out what people need.\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "3c4KAag0XDEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [3-5] Semantic functionとしてスキルJarvis（Plugin）にfunction名ChatとしてSenmantic Kernelに追加\n",
        "\n",
        "function_config = sk.SemanticFunctionConfig(prompt_config, prompt_template)\n",
        "chat_function = kernel.register_semantic_function(\n",
        "    skill_name=\"Jarvis\", function_name=\"Chat\", function_config=function_config\n",
        "    )\n"
      ],
      "metadata": {
        "id": "qGMBSnOUXDGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1-4] JarvisのChat機能の動作確認"
      ],
      "metadata": {
        "id": "8nt2H0vDaD5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KernelのPluginのfunctionに変数を渡す方法は\n",
        "\n",
        "- contextに埋め込む方法\n",
        "- ContextVariablesを使用する方法\n",
        "\n",
        "の2通りがあります。今回はContextVariablesを使います。\n",
        "\n"
      ],
      "metadata": {
        "id": "RRvxH_zOaMcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [1] 入力文の作成\n",
        "\n",
        "context_vars = sk.ContextVariables()\n",
        "user_input = \"お名前はなんですか？\"\n",
        "context_vars[\"user_input\"] = user_input\n"
      ],
      "metadata": {
        "id": "O9kyzPYaZwyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [2] 上記の入力を送って、回答を得ます\n",
        "\n",
        "answer = await kernel.run_async(chat_function, input_vars=context_vars)\n",
        "print(f\"Jarvis:> {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugsuG8KPZw06",
        "outputId": "69d07d63-ec29-4191-ec9b-e5249aad670b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jarvis:> 私の名前はジャービスです。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "messageのroll:systemの文章に、システムの名前としてJarvis（ジャービス）と記載しているので、きちんと自分の名前を答えてくれました。"
      ],
      "metadata": {
        "id": "C2ZGsBlebqXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [3] さらに会話を続けます\n",
        "\n",
        "context_vars[\"user_input\"] = \"私は小川雄太郎といいます。\"\n",
        "\n",
        "answer = await kernel.run_async(chat_function, input_vars=context_vars)\n",
        "print(f\"Jarvis:> {answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2a3BdR3Zw3R",
        "outputId": "8553979d-21b4-4d46-8f66-64c5e6add915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jarvis:> はじめまして、小川雄太郎さん。どのようなお手伝いができますか？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [4] さらに会話を続けます\n",
        "\n",
        "context_vars[\"user_input\"] = \"あれ、私の名前は何でしたっけ\"\n",
        "\n",
        "answer = await kernel.run_async(chat_function, input_vars=context_vars)\n",
        "print(f\"Jarvis:> {answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKDRYo8GZw5u",
        "outputId": "f294f6f1-be6b-4947-f944-6f226a8293b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jarvis:> おっしゃる通り、お名前は小川雄太郎さんですね。ご安心ください。何かお困りのことはありますか？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "現在は一往復だけのTextCompletionではなく、何往復もするChatCompletionを使用しているので、とくに会話履歴の保存操作などは、実装者側で明示的に行わなくても、ここまでの会話内容が残っています。そのため、私の名前を答えてくれます。"
      ],
      "metadata": {
        "id": "LPipLFPPcLAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1-5] 会話の初期化関数を定義（messagesを初期化する）"
      ],
      "metadata": {
        "id": "WFfux6wpcp9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "会話内容は変数messagesに蓄えられています。\n",
        "\n",
        "会話をリセットしたい場合には、変数messagesに追加された会話を消して、デフォルトで設定した最初の3つだけを残すようにします。"
      ],
      "metadata": {
        "id": "4f3Q8SEpcrns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [1] チャットの全体の長さ（messagesの長さ）をチェック\n",
        "print(len(prompt_template._messages))\n",
        "\n",
        "# -> 9　となるはずです。最初にdefault設定で、system、user、assistantの3つを追加しました\n",
        "# その後、会話を3往復したのので、3 + 3x2 = 9 です"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji9kZMLhcjek",
        "outputId": "ca54972b-6239-44a4-cfa6-dd45333cb64c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [2] messagesの中身を確認しましょう\n",
        "\n",
        "print(prompt_template._messages[7])\n",
        "print(\"------------\")\n",
        "print(prompt_template._messages[7][0])\n",
        "print(prompt_template._messages[7][1]._template)  # ._templateに追加されたstrが格納されています\n",
        "print(prompt_template._messages[8][0])\n",
        "print(prompt_template._messages[8][1]._template)  # ._templateに追加されたstrが格納されています\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MUxJmlvdJ7b",
        "outputId": "f3f36066-a6c5-4b2b-9d81-27d7de1eab32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('user', <semantic_kernel.semantic_functions.prompt_template.PromptTemplate object at 0x7f75efe903a0>)\n",
            "------------\n",
            "user\n",
            "あれ、私の名前は何でしたっけ\n",
            "assistant\n",
            "おっしゃる通り、お名前は小川雄太郎さんですね。ご安心ください。何かお困りのことはありますか？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "このように、prompt_template._messagesに、追加されているので、最初の3つだけを残す関数を定義して実行します。\n"
      ],
      "metadata": {
        "id": "1-JeTIqReF8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "【ここリセットする関数、自分で定義せずに、存在しているのかな？\n",
        "\n",
        "Semantic KernelはまだPythonはAPIのドキュメントすら整っていないので、分からない・・・】"
      ],
      "metadata": {
        "id": "TjOdCQ0ces0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [3] 初期の3つのmessage設定に戻す関数を定義します\n",
        "\n",
        "def init_chat_messages(prompt_template):\n",
        "    \"\"\"prompt_templateに溜まったmessagesを初期設定した最初の3つの状態に戻します\"\"\"\n",
        "    prompt_template._messages = prompt_template._messages[:3]\n"
      ],
      "metadata": {
        "id": "dmze4iiocjmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [4] 初期化の動作確認\n",
        "\n",
        "# 初期化実施\n",
        "init_chat_messages(prompt_template)\n",
        "print(len(prompt_template._messages))\n",
        "\n",
        "# 会話\n",
        "context_vars[\"user_input\"] = \"私の名前は何でしたっけ\"\n",
        "\n",
        "answer = await kernel.run_async(chat_function, input_vars=context_vars)\n",
        "print(f\"Jarvis:> {answer}\")\n",
        "\n",
        "# messagesの長さが3になりました。そして会話の履歴がなくなっているので、私の名前を答えることができなくなりました"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR39jAJBfGrf",
        "outputId": "5b6b8b72-4854-4ae0-cee1-46f889dae033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "Jarvis:> 私にはあなたの名前を知る能力はありません。もし教えていただければ、覚えます。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [5] 初期化の動作確認その2\n",
        "\n",
        "# 初期化実施\n",
        "init_chat_messages(prompt_template)\n",
        "print(len(prompt_template._messages))\n",
        "\n",
        "# 会話\n",
        "context_vars[\"user_input\"] = \"お名前はなんですか？\"\n",
        "\n",
        "answer = await kernel.run_async(chat_function, input_vars=context_vars)\n",
        "print(f\"Jarvis:> {answer}\")\n",
        "\n",
        "# 初期の3つの設定は残っているので、自分の名前は答えられます"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8pcCWy1fPDD",
        "outputId": "c7febe4e-7ff9-4360-f6a0-619d01505171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "Jarvis:> 私の名前はジャービスです。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [6] 最後に初期化\n",
        "init_chat_messages(prompt_template)"
      ],
      "metadata": {
        "id": "btA_XII3fWa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以上にて、「Semantic_Kernel」で「ChatGPT（gpt-35-turbo）」を（skill_name=\"Jarvis\", function_name=\"Chat\"）という設定のSemantic functionとして使用できるようになりました。\n",
        "\n",
        "<br>\n",
        "\n",
        "これで、Python言語のプログラム内で、ChatGPTと会話し放題です!\n",
        "\n",
        "（Azure OpenAIのAPI叩いているコストはかかります）。"
      ],
      "metadata": {
        "id": "F8O6H8hhfdYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "xtq85i9uf8wd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次回、第3回では、Jarvisの「Native Function」を作成し、起動のfunctionを作成します。そして今回作成したChat functionも含む、JarvisPluginを作成します。"
      ],
      "metadata": {
        "id": "BwuBFNwaf-9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 以上"
      ],
      "metadata": {
        "id": "_Jj32Xvd1S8A"
      }
    }
  ]
}